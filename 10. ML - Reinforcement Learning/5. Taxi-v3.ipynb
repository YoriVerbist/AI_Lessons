{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v3\n",
    "\n",
    "Gym provides different game environments which we can plug into our code and train an agent with. The library takes care of the API for providing all the information that our agent requires, like possible actions, score, and current state. We just need to focus on the algorithm part of our agent.\n",
    "\n",
    "We'll be using the Gym environment called Taxi-v3, which all of the details explained in previous Notebook were pulled from. The objectives, rewards, and actions are all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Gym\n",
    "\n",
    "We need to install Gym first. Executing the following should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.18.0.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow<=7.2.0\n",
      "  Downloading Pillow-7.2.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from gym) (1.5.3)\n",
      "Requirement already satisfied: future in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656445 sha256=2bbec43e8efc2575754d6423d67749e3b8bacc38d162e7d1c9f2aec8ebbdd614\n",
      "  Stored in directory: /home/yori/.cache/pip/wheels/d8/e7/68/a3f0f1b5831c9321d7523f6fd4e0d3f83f2705a1cbd5daaa79\n",
      "Successfully built gym\n",
      "Installing collected packages: Pillow, cloudpickle, pyglet, gym\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.0.1\n",
      "    Uninstalling Pillow-8.0.1:\n",
      "      Successfully uninstalled Pillow-8.0.1\n",
      "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 gym-0.18.0 pyglet-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gym's interface\n",
    "\n",
    "Once installed, we can load the game environment and render what it looks like. We can also print the action and state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space: Discrete(6)\n",
      "State Space: Discrete(500)\n",
      "\n",
      "Current state: 144\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env # load the game environment\n",
    "\n",
    "env.render() # visualize the environment\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State Space: {}\\n\".format(env.observation_space))\n",
    "\n",
    "print(\"Current state: %d\" % env.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of our problem:\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and dropoff locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination. In the illustration, these colors are reversed.\n",
    "\n",
    "- As verified by the prints, we have an Action Space of size 6 (south, north, east, west, pickup and dropoff) and a State Space of size 500 (the taxi's location x the passenger's location x the destination location).\n",
    "- The current state is randomly chosen (a state between 0 and 499)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the environment to a new, random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Current state: 251\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset the environment to a new, random state\n",
    "env.render()\n",
    "print(\"Current state: %d\" % env.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Back to our illustration\n",
    "\n",
    "We can actually take our illustration, encode its state, and give it to the environment to render in Gym.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 350px\"/>\n",
    "\n",
    "Recall that we have the taxi at row 3, column 1, our passenger is at location 2 (=Y), and our destination is location 0 (=R). Using the Taxi-v2 state encoding method, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger location, destination location)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial State - Exercise\n",
    "\n",
    "Generate the state where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B. What is the color of the taxi right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 419\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(4, 0, 4, 3) # (taxi row, taxi column, passenger location, destination location)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method\n",
    "\n",
    "The agent performs an action by using the step-method:\n",
    "\n",
    "```python\n",
    "state, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "Remember: There are 6 actions (0=south, 1=north, 2=east, 3=west, 4=pickup and 5=dropoff).\n",
    "\n",
    "The step-method returns:\n",
    "\n",
    "- state: the new state\n",
    "- reward: if your action was beneficial or not\n",
    "- done: indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "- info: additional info such as performance and latency for debugging purposes\n",
    "\n",
    "Let's go back to the illustrations state and try the different actions.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 250px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "New state: 328, reward: -1\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.s = state\n",
    "env.render()\n",
    "\n",
    "state, reward, done, info = env.step(3) # go west\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) # a -1 penalty for every wall hit and the taxi won't move anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state: 228, reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(1) # go north\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) # a slight negative reward for not making it to the destination\n",
    "\n",
    "env.render() # taxi moves up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method - Exercise\n",
    "\n",
    "Generate the state again where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.s = state\n",
    "env.render()\n",
    "\n",
    "state, reward, done, info = env.step(3) # go west\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) # a -1 penalty for every wall hit and the taxi won't move anywhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now carry out the two steps to drive the taxi to the destination location and drop off the passenger. Render at each step the environment, print the new state number, the reward and done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called *P*. We can think of it like a matrix that has the number of states as rows and the number of actions as columns, i.e. a states × actions matrix.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned for state 479."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 479, -1, False)],\n",
       " 1: [(1.0, 379, -1, False)],\n",
       " 2: [(1.0, 499, -1, False)],\n",
       " 3: [(1.0, 479, -1, False)],\n",
       " 4: [(1.0, 479, -10, False)],\n",
       " 5: [(1.0, 475, 20, True)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.s = 479\n",
    "env.render()\n",
    "\n",
    "env.P[479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure\n",
    "\n",
    "``\n",
    "{action: [(probability, nextstate, reward, done)]}.\n",
    "``\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state.\n",
    "- In this environment, the probability of each action is always 1.0.\n",
    "- The nextstate is the state we would be in if we take the action at this index of the dictionary.\n",
    "- All the movement actions have a -1 reward, the pickup action has a -10 reward, the dropoff actions has +20 reward in this particular state.\n",
    "- done is used to tell us when we have successfully dropped off a passenger at the right location (action 5 in this state). Each successful dropoff is the end of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solving the environment without Reinforcement Learning\n",
    "\n",
    "Let's see what will happen if we try to brute-force our way to solving the problem without RL.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The `env.action_space.sample()` method automatically selects one random action from the set of all possible actions.\n",
    "\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 4199\n",
      "Penalties incurred: 1344\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now show the different steps our taxi carried out, picking up the passenger and delivering him to the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 4199\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.02)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. Our agent takes thousands (?) of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Q-learning\n",
    "\n",
    "We are going to use a simple RL algorithm called Q-learning which will give our agent some memory. Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "Therefore the Q-learning algorithm uses a Q-Table. The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). The matrix is first initialized to 0, and then values are updated during training.\n",
    "\n",
    "<img src=\"./resources/qtable.png\" style=\"height: 650px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action at every state is the action with the highest Q-value. So for state 328 the highest value is -1.971 (=North). For state 499 the highest value is 29 (=West). These actions indeed seem to be the best options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "North is best?\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "West is best?\n"
     ]
    }
   ],
   "source": [
    "env.s = 328\n",
    "env.render()\n",
    "print(\"North is best?\\n\")\n",
    "\n",
    "env.s = 499\n",
    "env.render()\n",
    "print(\"West is best?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training the Agent\n",
    "\n",
    "First, we'll initialize the Q-table to a 500×6 matrix of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over 100 000 of episodes (of course you don't need to write this algorithm yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 36.2 s, sys: 7.41 s, total: 43.6 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100 000 episodes, let's see what the Q-values are for our two states. Are North and West indeed the two most preferable moves for the two states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.4043024   -2.27325184  -2.41131299  -2.36087579 -10.60118881\n",
      " -11.06768132]\n",
      "[ 1.85606096  1.36966912  3.77152136 11.         -2.59489135 -2.72256117]\n"
     ]
    }
   ],
   "source": [
    "print(q_table[328])\n",
    "print(q_table[499])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating the agent\n",
    "\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.96\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state]) # take the action with the highest q-value\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation that the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Solving the environment with Reinforcement Learning - Exercise\n",
    "\n",
    "Now that our agent is trained, use the q-table to solve the taxi problem with 328 as the initial state (by analogy with 7. Solving the environment without RL). Put every frame in a dictionary for later animation. Print the timesteps taken and penalties incurred. Pretty impressive! No?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3448\n",
      "Penalties incurred: 1123\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 3448\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.02)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the different steps our taxi carried out, picking up the passenger and delivering him to the destination. Wait 0.5 seconds between two frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Solving the environment with RL - Exercise\n",
    "\n",
    "Solve the problem for these two initial states.\n",
    "\n",
    "<img src=\"./resources/taxi1.png\" style=\"height: 300px\"/>\n",
    "<img src=\"./resources/taxi2.png\" style=\"height: 300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the problem with the first/second initial state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the solution\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
