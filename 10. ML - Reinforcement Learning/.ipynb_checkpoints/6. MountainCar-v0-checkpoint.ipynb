{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar-v0 - Exercise\n",
    "\n",
    "The Mountain Car problem is described as follows: a car is on a one-dimensional track, positioned between two mountains. The goal is to drive up the mountain on the right; however, the car’s engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State space\n",
    "\n",
    "The car’s state, at any point in time, is given by a vector containing its horizontal position and velocity. The car commences each episode stationary, at the bottom of the valley between the hills (at position approximately -0.5), and the episode ends when either the car reaches the flag (position  0.6) or after 200 moves.\n",
    "\n",
    "<img src=\"./resources/mountaincar.png\" style=\"height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the game environment and render what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0') # load the game environment\n",
    "env.reset()\n",
    "\n",
    "env.render() # visualize the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment opens in a different window. The environment works best if you shrink your browser a little bit and place the window with the car beside it. To close this window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First open the environment again. Now we can ask the car's state and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State Space: {}\\n\".format(env.state))\n",
    "print(\"Position: %f \" % env.state[0])\n",
    "print(\"Velocity: %f \" % env.state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the state is a vector containing its horizontal position and velocity. Check in the figure if the car is indeed at the given horizontal position. The velocity should be zero. Right?\n",
    "\n",
    "## 2. Reset the environment - Exercise\n",
    "\n",
    "Try to reset the environment to a new random state, render it and print the state again. Do you see the car at a different location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to a new, random state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action space\n",
    "\n",
    "At each move, the car has three possible actions: push left (=0), push right (=2) or do nothing (=1), and a penalty of 1 unit is applied for each move taken (including doing nothing). This means that, unless the car can figure out a way to ascend the mountain in less than 200 moves, it will always achieve a total *reward* of -200 units.\n",
    "\n",
    "The car performs an action by using the step-method:\n",
    "\n",
    "```python\n",
    "state, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "As you know the episode ends (done will be True) after 200 moves. We will try to push the car to the right at each move and render the environment at each step. We will also count the number of steps and print it. As you can see, the car's engine is not strong enough to drive up the mountain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(2)\n",
    "    env.render()\n",
    "    step = step + 1\n",
    "    clear_output(wait=True)\n",
    "    print(\"Steps: %d\" % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random actions - Exercise\n",
    "\n",
    "Maybe you can try to reach the flag by performing a random action at each step? Any success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving the problem with a random action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solving the environment without Reinforcement Learning - Exercise\n",
    "\n",
    "Can you figure out a strategy to reach the flag within 200 moves? Try to code your strategy! Count the number of steps. My best try was reaching the flag in 89 steps. Can you do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving the problem without reinforcement learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solving the environment with Reinforcement Learning\n",
    "\n",
    "So far, we have driven the car ourselves. This of course has nothing to do with Reinforcement Learning. The code below will use the Q-learning algorithm to let the agent use the environment's rewards to learn, over time, to take the best action to drive up the mountain. This code is just to demonstrate. This won't be an exam question.\n",
    "\n",
    "For tracking purposes, the code returns a list containing the average total reward for each run of 100 episodes. It visualizes the movements of the Mountain Car only for the final 10 episodes using the `env.render()` method. The environment is only visualized for the final 10 episodes, rather than for all episodes, because visualizing the environment dramatically increases the code run time. So don't panic if the car doesn't move in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            # Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "                \n",
    "    return ave_reward_list\n",
    "\n",
    "# run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average reward vs the episode number for the 5000 episodes, we can see that, initially, the average reward is fairly flat, with each run terminating once the maximum 200 movements is reached. This is the exploration phase of the algorithm. Only after 4000 episodes, the agent is making progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to close the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
