{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Speech Recognition work?\n",
    "\n",
    "Let's take a moment to talk about how Speech Recognition works. Speech Recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made Speech Recognition accurate enough. A full discussion would fill a book, so we won’t bore you with all of the technical details here.\n",
    "\n",
    "## 1. Turning Sounds into Bits\n",
    "\n",
    "The first step in Speech Recognition is obvious — we need to feed sound waves into a computer. As we've already learned, an image can be seen as an array of numbers so that we can feed it directly into a neural network for image recognition:\n",
    "\n",
    "<img src=\"./resources/8.png\"  style=\"height: 250px\"/>\n",
    "\n",
    "But sound is transmitted as waves. How do we turn sound waves into numbers so that it can be used as input for a neural network?\n",
    "\n",
    "<img src=\"./resources/SR.png\"  style=\"height: 150px\"/>\n",
    "\n",
    "First let's install the Python library playsound. The module contains only one thing - the function (also named) playsound. It requires one argument - the path to the file with the sound you’d like to play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'hello_male_mono.wav'\n",
    "fullname = 'resources/' + file\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "playsound(fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the sound clip of someone saying \"Hello\":\n",
    "\n",
    "<img src=\"./resources/hello.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let’s zoom in on one tiny part of the sound wave and take a look:\n",
    "\n",
    "<img src=\"./resources/wave.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "To turn this sound wave into numbers, we use *sampling*: we just record the height of the wave at equally-spaced points in time (thousands of times a second):\n",
    "\n",
    "<img src=\"./resources/sampling.gif\"  style=\"height: 200px\"/>\n",
    "\n",
    "This means that we are inspecting the wave __thousands of times a second__ and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is. *CD Quality* audio is sampled at 44.1khz (44,100 readings per second). But for Speech Recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech.\n",
    "\n",
    "Let's sample our \"Hello\" sound wave 16,000 times per second. Here are the first 100 samples. Each number represents the amplitude (= height) of the sound wave at 1/16000th of a second intervals:\n",
    "\n",
    "<img src=\"./resources/sample.png\"  style=\"height: 75px\"/>\n",
    "\n",
    "We can use pyplot to plot the sound wave for our .wav-file. As you can see, our .wav-file is sampled at 44.1khz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pyplot and wavfile modules \n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# read the wav file (mono)\n",
    "samplingFrequency, signalData = wavfile.read(fullname)\n",
    "\n",
    "print('Sampling Frequency: ' + str(samplingFrequency))\n",
    "print('Number of samples: ' + str(len(signalData)))\n",
    "print('Length of ' + file + ': ' + str(round(len(signalData)/samplingFrequency,2)) + ' seconds\\n')\n",
    "\n",
    "# print the first 100 amplitudes\n",
    "print(signalData[:100])\n",
    "\n",
    "# plot the signal read from wav file\n",
    "plot.title('Sound wave of ' + file)\n",
    "plot.plot(signalData)\n",
    "plot.xlabel(file)\n",
    "plot.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing our Sampled Sound Data\n",
    "\n",
    "We now have an array of numbers with each number representing the sound wave’s amplitude at 1/16,000th of a second intervals.\n",
    "We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data.\n",
    "\n",
    "Let’s start by grouping our sampled audio into 20-millisecond-long chunks. Here’s our first 20 milliseconds of audio (i.e., our first 320 samples):\n",
    "\n",
    "<img src=\"./resources/20ms.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time:\n",
    "\n",
    "<img src=\"./resources/20msg.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "This recording is only 1/50th of a second (20 milliseconds) long. But even this short recording is a complex mish-mash of different frequencies of sound. There are some low sounds, some mid-range sounds, and even two peaks of high-pitched sounds sprinkled in. Taken all together, these different frequencies mix together to make up the complex sound of human speech.\n",
    "\n",
    "You can compare the complex mish-mash of different frequencies with someone playing a C Major chord on a piano. That sound is the combination of three musical notes — C, E and G — all mixed together into one complex sound.\n",
    "\n",
    "Now the idea is to break apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one. We do this using a mathematical operation called a Fourier Transformation.\n",
    "\n",
    "The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip. There are 160 numbers in the result (8000hz divided into frequency ranges of 50hz):\n",
    "\n",
    "<img src=\"./resources/ft.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "This is a lot easier to see when you draw this as a chart. You can see that our 20 millisecond sound snippet has a lot of low-frequency energy and not much energy in the higher frequencies. That’s typical of \"male\" voices:\n",
    "\n",
    "<img src=\"./resources/ft20ms.png\"  style=\"height: 100px\"/>\n",
    "\n",
    "If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk):\n",
    "\n",
    "<img src=\"./resources/all.png\"  style=\"height: 400px\"/>\n",
    "\n",
    "A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network. Since a spectrogram almost looks like a normal image, we can use a Convolutional Neural Network to perform the classification task.\n",
    "\n",
    "You've made it to the end of this Notebook! Well done!\n",
    "\n",
    "To finish this Notebook, let's plot the spectrogram for our .wav-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.specgram(signalData,Fs=samplingFrequency)\n",
    "\n",
    "plot.xlabel('Time')\n",
    "plot.ylabel('Frequency')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Download a .wav-file (or record one yourself) and try to visualize the sound wave. Plot the spectrogram (you need to convert the .wav-file to mono first)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
