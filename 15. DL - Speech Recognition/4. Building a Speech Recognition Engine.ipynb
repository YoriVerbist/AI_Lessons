{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Speech Recognition Engine in Keras\n",
    "\n",
    "In this Notebook we will train a Convolutional Neural Network by feeding it speech signals (actually one word). After training we will try to predict the corresponding text of a .wav-file.\n",
    "\n",
    "The task will be to classify audio between three classes: bed, cat and happy.  In this \n",
    "<a href=\"http://taiwan.thomasmore.be/pr2/koen/bedcathappy.rar\">rar-file</a> you will find three folders. Download this rar-file and extract the folders in a sub-folder `data`. The name of the sub-folders is actually the label of the audio files in it. Each folder contains approximately 1700 audio files. Play some audio files randomly to get an overall idea.\n",
    "\n",
    "<img src=\"./resources/single.png\"  style=\"height: 200px\"/>\n",
    "\n",
    "As you know from the previous Notebook, directly feeding a speech signal to a ConvNet model won't do the job. There are some preprocessing steps you'll need to take. Basically we turn sound waves into numbers so that they can be used as input for a neural network.\n",
    "\n",
    "## 1. Preprocess our sound data\n",
    "\n",
    "In the previous Notebook we talked about the Fourier Transformation to transform our sound wave into a spectrogram. There are actually two ways to calculate such a spectrogram: MFCC (Mel Frequency Cepstral Coefficients) and FTT (Fast Fourier Transformation). The code below will use the first technique to preprocess the .wav-files in our three folders into spectrograms and create our test and training set.\n",
    "\n",
    "First install Librosa, a Python package for music and audio analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa==0.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary Keras modules and a Python code library with some functions we will use later. By the way, you don't need to know the details of this code library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('library/preprocess.py')\n",
    "from library.preprocess import *\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the code below to preprocess our sound data. Since computing MFCC is time consuming, we will do it only once and save the computed values in a .npy file which is named after the name of the label. After running the code you can find those .npy-files in the root folder of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# save data to array file first\n",
    "save_data_to_array(max_len=feature_dim_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the train and test set\n",
    "\n",
    "We'll take advantage of scikit-learn function `train_test_split` which will automatically split the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "# feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "\n",
    "# reshaping to perform 2D convolution, there is only one channel (normally for images 3: RGB)\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "# one hot encoding (already explained in the Computer Vision Lesson, CIFAR-10 )\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model and train it\n",
    "\n",
    "Finally it is time to build our CNN and train it with the train data. The code below has no secrets anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 100\n",
    "verbose = 1\n",
    "\n",
    "class_names =  ['bed', 'cat', 'happy']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict\n",
    "\n",
    "In the folder `data_test` you will find some .wav-files to test your model with. Since we achieved an accuracy of 94%, all these files should predict the correct corresponding text. Maybe you can try if this is still the case if you pronounce the words yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = wav2mfcc('./data_test/bed_0.wav')\n",
    "sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "predicted_index = np.argmax(model.predict(sample_reshaped))\n",
    "\n",
    "print(class_names[predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise\n",
    "\n",
    "In the `data_fruit` folder you will find another dataset for Speach Recognition. Use this dataset as input and build and train a model. Can you achieve a high accuracy? Try to predict your own words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
