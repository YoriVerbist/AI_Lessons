{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Sentiment Analyzer\n",
    "\n",
    "\n",
    "Sentiment analysis is the process of determining the sentiment of a given piece of text and is also referred to as Opinion Mining. It is one of the most popular applications of Natural Language Processing and it's mostly used in social media and customer reviews data. \n",
    "\n",
    "In this Notebook we are using sentiment analysis to determine whether a movie review is positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and download the data\n",
    "\n",
    "We will use the NLTK's movie_reviews corpus as our labeled training data. The movie_reviews corpus contains 2K movie reviews with sentiment classification. We're going to use the Naive Bayes classifier. This is a pretty popular classifier used in text classification, sentiment analysis, spam filtering, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the data as follows. We will also download the English stopwords for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data\n",
    "\n",
    "We have two categories for classification: positive and negative. The movie_reviews corpus has already categorized the reviews  as positive or negative. As you can see there are 1,000 positive reviews and 1,000 negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review categories\n",
    "print (movie_reviews.categories())\n",
    "\n",
    "# total reviews\n",
    "print (len(movie_reviews.fileids()))\n",
    " \n",
    "# total positive reviews\n",
    "print (len(movie_reviews.fileids('pos')))\n",
    " \n",
    "# total negative reviews\n",
    "print (len(movie_reviews.fileids('neg')))\n",
    " \n",
    "# print the name of the first positive review file\n",
    "positive_review_file = movie_reviews.fileids('neg')[0] \n",
    "print (positive_review_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the content of a file. We can obtain all words in a review with the words(review_file)-method. Using the words()-method without any parameter, would return the words in all movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in movie_reviews.words('neg/cv000_29416.txt'):\n",
    "    print(word, end = ' ') # use a space instead of a linefeed after each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a list of documents\n",
    "\n",
    "First, execute the code below. We iterate over the two categories ('neg' and 'pos') and take all of the file IDs (each review has its own review file). Then we'll store the word_tokenized version (a list of words) of the file ID, followed by the positive or negative label, in one big list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just so you can see the outcome of the code above, we print out documents[0]: the first element is a list of words (from the first file), and the 2nd element is the \"pos\" or \"neg\" label (look at the end of the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use random to shuffle our documents. This is because we're going to train and test. If we left them in order, we'd train on all the negatives, some positives, and then test only against positives. We don't want that, so we shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect the top 3,000 words\n",
    "\n",
    "Now, we want to collect all words that we found, so we have a massive list of typical words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can perform a frequency distribution, to find out the most common words. As you will see, the most popular \"words\" are actually things like punctuation, \"the,\" \"a\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = nltk.FreqDist(all_words)\n",
    "print(word_frequency.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to clean up things a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string \n",
    " \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "words_clean = []\n",
    " \n",
    "for word in all_words:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords_english and word not in string.punctuation:\n",
    "        words_clean.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out how many occurrences a word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = nltk.FreqDist(words_clean)\n",
    "print(word_frequency.most_common(100))\n",
    "print(word_frequency[\"stupid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. Let's make a new variable, top_words, which contains the top 3,000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(word_frequency.keys())[:3000]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the featureset\n",
    "\n",
    "We're going to build a function that will find these top 3,000 words in our positive and negative documents, marking their presence either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_words(words):\n",
    "    wordset = set(words)\n",
    "    result = {}\n",
    "    for w in top_words:\n",
    "        result[w] = (w in wordset) # true if top_word is occurring in the wordset\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can create an object with all the top 3,000 words and an indication whether the word is present in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((find_top_words(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can do this for all of our documents, saving the word existence booleans and their respective positive or negative categories (have a look at the end of the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "for (words, category) in documents:\n",
    "    featuresets.append((find_top_words(words), category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are using the top 3,000 words as input features for our classifier (the value of the feature is a boolean indicating whether the word exists in the document). The output feature or label is \"pos\" or \"neg\" indicating whether the review is positive of negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the classifier \n",
    "\n",
    "Now it is time to choose an algorithm, separate our data into training and testing sets, and press go! The algorithm that we're going to use is the Naive Bayes classifier. This is a pretty popular algorithm used in text classification, sentiment analysis, spam filtering, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set that we'll train our classifier with\n",
    "training_set = featuresets[:1900]\n",
    "\n",
    "# testing set that we'll test against.\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Naive Bayes classifier using the training data and compute the accuracy using the inbuilt method available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print('\\nAccuracy of the classifier:', nltk_accuracy(classifier, testing_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take it a step further to see what the most valuable words are when it comes to positive or negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the term \"sucks\" appears 10.7 more times as often in negative reviews as it does in positive reviews. You might get another value since we randomly shuffled our documents before splitting the train and test data.\n",
    " \n",
    "We also can print the most informative features in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "print('\\nTop ' + str(N) + ' most informative words:')\n",
    "for i, item in enumerate(classifier.most_informative_features(N)):\n",
    "    print(str(i+1) + '. ' + item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the classifier with custom reviews\n",
    "\n",
    "We provide custom review texts and check the classification output of the trained classifier. Will the classifier correctly predict both negative and positive reviews provided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input movie reviews\n",
    "input_reviews = [\n",
    "    'The costumes in this movie were great.',\n",
    "    'I think the story was terrible and the characters were very weak.',\n",
    "    'People say that the director of the movie is amazing. It was a wonderful movie.',\n",
    "    'This is such an idiotic movie. I will not recommend it to anyone.',\n",
    "    'It doesn\\'t matter how much you enjoy kung-fu and karate films: with 47 Ronin, you\\'re better off saving your money, your popcorn, and time.',\n",
    "    'Majili is the rare movie that succeeds fully on almost every level, where each character, scene, costume, and joke firing on all cylinders to make this great film worth repeated viewings.',\n",
    "    'Despite a compelling lead performance by Tom Hanks, Forrest Gump never gets out of the shadow of its weak plot and questionable crappy premise.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            words_clean.append(word)\n",
    "    \n",
    "    words_dictionary = dict([word, True] for word in words_clean)  \n",
    "    return words_dictionary\n",
    "\n",
    "print(\"\\nMovie review predictions:\")\n",
    "for review in input_reviews:\n",
    "    print(\"\\nReview:\", review)\n",
    "\n",
    "    # Compute the probabilities\n",
    "    probabilities = classifier.prob_classify(cleanup_words(review))\n",
    "\n",
    "    # Pick the maximum value\n",
    "    predicted_sentiment = probabilities.max()\n",
    "\n",
    "    # Print outputs\n",
    "    print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "    print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
