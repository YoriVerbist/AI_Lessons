{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Beginners using NLTK and spaCy\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, Open Source, easy to use, well documented and it has a large community. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analyse, preprocess and understand written text.\n",
    "\n",
    "You can install it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizing\n",
    "\n",
    "When we deal with text, we need to break it down into smaller pieces for analysis. This is\n",
    "where tokenization comes into the picture. It is the process of dividing the input text into a\n",
    "set of pieces like words or sentences. These pieces are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\"\n",
    "\n",
    "# sentence tokenizer\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are considered as noise in the text. Text may contain stopwords such as *is, am, are, this, a, an, the,* etc.\n",
    "\n",
    "It is clear that you first need a list of stopwords so these words can be removed. This list can be easily created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all Dutch stopwords -  Exercise\n",
    "\n",
    "Write a little program that prints all Dutch stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stopwords in dutch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords and punctuation - Exercise\n",
    "\n",
    "Now write a function `words()` that has a string as input parameter and returns all the words in that string without the stopwords. Also get rid of punctuation. The output of the input_text above should be:\n",
    "\n",
    "```\n",
    "Words without stopwords:  ['know', 'tokenization', 'works', 'actually', 'quite', 'interesting', 'let', 'analyze', 'couple', 'sentences', 'figure']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def words (input_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    output = []\n",
    "    for word in tokenizer.tokenize(input_text):\n",
    "        if word.lower() not in stop_words:\n",
    "            output.append(word.lower())\n",
    "    return output\n",
    "\n",
    "print(\"Words without stopwords: \", words(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "When working with text, we have to deal with different forms of the same word. For example, the word *sing* can appear in many forms such as *sang, singer, singing, singer,* and so on. When we analyze text, it's useful to reduce words in their different forms into a base form. This will enable us to extract useful statistics to analyze the input text.\n",
    "\n",
    "Stemming is one way to achieve this. It is basically a process that cuts off the ends of words to extract their base forms. Let's see how to do it using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'SNOWBALL', 'LANCASTER']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), '\\n', '='*68)\n",
    "\n",
    "# stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), snowball.stem(word), lancaster.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemmers above is the level of strictness that's used to arrive at the base form. The Porter stemmer is the least in terms of strictness (\"possibly\" becomes \"possibl\") and Lancaster is the strictest (\"possibly\" becomes \"poss\").\n",
    "\n",
    "Note that the result might not be an actual word. All the three stemmers said that the base form of \"calves\" is \"calv\", which is not a real word.\n",
    "\n",
    "On the other hand all the three stemmers reduced \"connections, connected, connecting\" to a correct common word \"connect\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization - Exercise\n",
    "\n",
    "Lemmatization is another way of reducing words to their base form. The lemmatization process uses a vocabulary and morphological analysis of words. It obtains the base forms by removing word endings such as ing or ed. This\n",
    "base form of a word is known as a lemma. If you lemmatize the word \"calves\", you\n",
    "should get \"calf\" as the output. One thing to note is that the output depends on whether the word is a verb or a noun.\n",
    "\n",
    "Before using lemmatization, we have to download WordNet, a large lexical database of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a little program to lemmatize the same `input_words` as above. Use the `lemmatize`-method from the `WordNetLemmatizer`-class. This method has two parameters: the first parameter is the word to be lemmatized, the second parameter is the type of output (pos='n' for a noun lemma, pos='v' for a verb lemma). The output should be something like this:\n",
    "\n",
    "```\n",
    "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
    " ===========================================================================\n",
    "                 writing                 writing                   write\n",
    "             connections              connection             connections\n",
    "               connected               connected                 connect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the noun lemmatizer works differently than the verb lemmatizer when it\n",
    "comes to words like writing or calves. If you compare these outputs to stemmer outputs, you\n",
    "will see that there are differences too. The lemmatizer outputs are all meaningful whereas\n",
    "stemmer outputs may or may not be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS Tagging\n",
    "\n",
    "The target of Part-of-Speech (POS) Tagging is to identify the grammatical group of a given word, whether it is a noun, pronoun, adjective, verb, adverb, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "We will use spaCy, a different Python library for NLP because it gives better results than NLTK for POS Tagging and Named Entity Recognition. First install spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next install the English model (restart the Kernel afterwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the core spaCy English model and create a spaCy document that we will be using to perform Part-of-Speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaCy document object has several attributes that can be used to perform a variety of tasks. For instance, to print the text of the document, the text attribute is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sen.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the pos_ attribute returns the POS tag. And finally, to get the explanation of the POS tag, we can use the spacy.explain() method and pass it the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sen[7])\n",
    "print(sen[7].pos_)\n",
    "print(spacy.explain(sen[7].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print all the POS tags (we've improved the readability by adding 12 spaces between the text and the POS tag and then another 10 spaces between the POS tags and the explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {spacy.explain(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing about spaCy is, that you can use the dependency visualizer to show Part-of-Speech tags and syntactic dependencies. Maybe you can try some other sentences to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")\n",
    "displacy.render(sen, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging in Dutch - Exercise\n",
    "\n",
    "POS Tagging can be done in Dutch as well. You will probably have to install the Dutch model.\n",
    "\n",
    "Use this sentences as input: \"De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.\" The output should be as follows:\n",
    "\n",
    "```\n",
    "De           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "concentratie NOUN       N|soort|ev|neut__Number=Sing\n",
    "broeikasgassen ADP        Prep|voor__AdpType=Prep\n",
    "die          PRON       Pron|aanw|neut|attr__PronType=Dem\n",
    "bijdragen    NOUN       N|soort|mv|neut__Number=Plur\n",
    "aan          ADP        Prep|voor__AdpType=Prep\n",
    "de           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "verandering  NOUN       N|soort|ev|neut__Number=Sing\n",
    "van          ADP        Prep|voor__AdpType=Prep\n",
    "het          DET        Art|bep|onzijd|neut__Definite=Def|Gender=Neut|PronType=Art\n",
    "klimaat      NOUN       N|soort|ev|neut__Number=Sing\n",
    ",            PUNCT      Punc|komma__PunctType=Comm\n",
    "heeft        VERB       V|hulp|ott|3|ev__Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
    "opnieuw      ADV        Adv|gew|geenfunc|stell|onverv__Degree=Pos\n",
    "een          DET        Art|onbep|zijdofonzijd|neut__Definite=Ind|Number=Sing|PronType=Art\n",
    "recordhoogte NOUN       N|soort|ev|neut__Number=Sing\n",
    "bereikt      VERB       V|trans|verldw|onverv__Subcat=Tran|Tense=Past|VerbForm=Part\n",
    ".            PUNCT      Punc|punt__PunctType=Peri\n",
    "\n",
    "```\n",
    "\n",
    "It is not possible to explain the POS tag in Dutch. Just use tag_ in the third column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-2.2.0/nl_core_news_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "sen = sp('De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition refers to the identification of words in a sentence as an entity e.g. the name of a person, place, organization, etc. Let's see how the spaCy library performs Named Entity Recognition. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million.')\n",
    "\n",
    "print(sen.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that three named entities were identified. To see the detail of each named entity, you can use the text, label, and the spacy.explain method which takes the entity object as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in sen.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the POS tags, we can also view named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million. David wants 100 Million Dollars.')\n",
    "displacy.render(sen, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
