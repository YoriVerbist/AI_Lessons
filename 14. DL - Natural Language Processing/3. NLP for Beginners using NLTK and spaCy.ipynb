{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Beginners using NLTK and spaCy\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, Open Source, easy to use, well documented and it has a large community. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analyse, preprocess and understand written text.\n",
    "\n",
    "You can install it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (3.5)\r\n",
      "Requirement already satisfied: joblib in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from nltk) (0.17.0)\r\n",
      "Requirement already satisfied: tqdm in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from nltk) (4.51.0)\r\n",
      "Requirement already satisfied: click in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: regex in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from nltk) (2020.11.13)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yori/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/yori/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yori/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizing\n",
    "\n",
    "When we deal with text, we need to break it down into smaller pieces for analysis. This is\n",
    "where tokenization comes into the picture. It is the process of dividing the input text into a\n",
    "set of pieces like words or sentences. These pieces are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\"\n",
    "\n",
    "# sentence tokenizer\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are considered as noise in the text. Text may contain stopwords such as *is, am, are, this, a, an, the,* etc.\n",
    "\n",
    "It is clear that you first need a list of stopwords so these words can be removed. This list can be easily created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be', 'were', 'wasn', 'our', 'who', 'with', 'shan', 'himself', 'ain', 'did', \"you'd\", 'ma', 'again', 'any', 'themselves', 'few', 'll', 'off', 'above', 'up', \"hasn't\", 're', 'at', 'not', 'ours', \"mightn't\", 'it', 'then', 'out', 'mustn', 'for', 'nor', \"couldn't\", 'very', \"you'll\", 'those', 'into', 'here', 'from', 'can', 'own', 'yourself', 'more', \"don't\", 'when', 'my', 'your', 'whom', 'all', 'only', 'where', 'm', 'against', 'these', 've', 'am', 'me', 'while', 'before', 'a', 'their', 'does', \"aren't\", 'hadn', \"wasn't\", 'now', 'y', 'between', 'do', 'or', 'having', 'most', 'd', 'but', 'both', 'won', 'mightn', \"isn't\", 'in', 'he', 'which', 'than', 'was', 'her', 'yours', 'by', 'isn', 'o', \"mustn't\", 'itself', 'yourselves', 'same', 'i', 'is', 'if', 'to', 'on', 'the', 'aren', \"should've\", 'weren', 'once', 'through', 'wouldn', \"weren't\", \"you're\", 'are', \"needn't\", 'after', 'been', 'hers', 'why', 'other', 'just', 'them', 'needn', \"shouldn't\", 'its', 'doesn', 'herself', \"won't\", 'will', 'as', 'until', \"doesn't\", 'because', 'we', 'they', 'there', \"that'll\", 'ourselves', 'myself', 'don', 'this', \"wouldn't\", 'have', \"didn't\", \"hadn't\", 'being', \"you've\", 'couldn', \"it's\", \"she's\", 'had', 'over', 'didn', 'shouldn', 'him', \"haven't\", 'under', 'she', 'of', 'what', 'hasn', 'that', 'how', 'such', 'haven', 'his', 'below', 'theirs', 'doing', 'no', 'should', 'down', 'so', 's', 'further', 'too', 'each', 'some', 'an', 't', 'has', \"shan't\", 'about', 'during', 'and', 'you'}\n"
     ]
    }
   ],
   "source": [
    "# create stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all Dutch stopwords -  Exercise\n",
    "\n",
    "Write a little program that prints all Dutch stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ze', 'ik', 'een', 'heb', 'alles', 'niets', 'tegen', 'kan', 'met', 'doen', 'nog', 'haar', 'hebben', 'voor', 'door', 'zonder', 'wordt', 'die', 'ge', 'u', 'uw', 'ja', 'kon', 'eens', 'me', 'deze', 'werd', 'altijd', 'niet', 'moet', 'geweest', 'der', 'geen', 'wil', 'op', 'worden', 'want', 'in', 'iets', 'iemand', 'was', 'zelf', 'is', 'tot', 'het', 'hoe', 'heeft', 'al', 'de', 'wat', 'doch', 'naar', 'onder', 'wezen', 'hem', 'waren', 'toen', 'te', 'hier', 'zijn', 'dit', 'hun', 'meer', 'van', 'om', 'veel', 'zij', 'uit', 'en', 'dan', 'mijn', 'je', 'zal', 'had', 'over', 'bij', 'daar', 'dus', 'kunnen', 'ook', 'nu', 'of', 'na', 'hij', 'wie', 'zou', 'ons', 'omdat', 'toch', 'andere', 'mij', 'men', 'dat', 'zich', 'als', 'aan', 'er', 'reeds', 'maar', 'ben', 'zo'}\n"
     ]
    }
   ],
   "source": [
    "# print stopwords in dutch\n",
    "print(set(stopwords.words('dutch')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords and punctuation - Exercise\n",
    "\n",
    "Now write a function `words()` that has a string as input parameter and returns all the words in that string without the stopwords. Also get rid of punctuation. The output of the input_text above should be:\n",
    "\n",
    "```\n",
    "Words without stopwords:  ['know', 'tokenization', 'works', 'actually', 'quite', 'interesting', 'let', 'analyze', 'couple', 'sentences', 'figure']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without stopwords:  ['know', 'tokenization', 'works', 'actually', 'quite', 'interesting', 'let', 'analyze', 'couple', 'sentences', 'figure']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def words (input_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    output = []\n",
    "    for word in tokenizer.tokenize(input_text):\n",
    "        if word.lower() not in stop_words:\n",
    "            output.append(word.lower())\n",
    "    return output\n",
    "\n",
    "print(\"Words without stopwords: \", words(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "When working with text, we have to deal with different forms of the same word. For example, the word *sing* can appear in many forms such as *sang, singer, singing, singer,* and so on. When we analyze text, it's useful to reduce words in their different forms into a base form. This will enable us to extract useful statistics to analyze the input text.\n",
    "\n",
    "Stemming is one way to achieve this. It is basically a process that cuts off the ends of words to extract their base forms. Let's see how to do it using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER        SNOWBALL       LANCASTER \n",
      " ====================================================================\n",
      "         writing           write           write            writ\n",
      "     connections         connect         connect         connect\n",
      "       connected         connect         connect         connect\n",
      "      connecting         connect         connect         connect\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl         possibl            poss\n",
      "       provision          provis          provis          provid\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchi        scratchy\n",
      "          calves            calv            calv            calv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'SNOWBALL', 'LANCASTER']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), '\\n', '='*68)\n",
    "\n",
    "# stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), snowball.stem(word), lancaster.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemmers above is the level of strictness that's used to arrive at the base form. The Porter stemmer is the least in terms of strictness (\"possibly\" becomes \"possibl\") and Lancaster is the strictest (\"possibly\" becomes \"poss\").\n",
    "\n",
    "Note that the result might not be an actual word. All the three stemmers said that the base form of \"calves\" is \"calv\", which is not a real word.\n",
    "\n",
    "On the other hand all the three stemmers reduced \"connections, connected, connecting\" to a correct common word \"connect\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization - Exercise\n",
    "\n",
    "Lemmatization is another way of reducing words to their base form. The lemmatization process uses a vocabulary and morphological analysis of words. It obtains the base forms by removing word endings such as ing or ed. This\n",
    "base form of a word is known as a lemma. If you lemmatize the word \"calves\", you\n",
    "should get \"calf\" as the output. One thing to note is that the output depends on whether the word is a verb or a noun.\n",
    "\n",
    "Before using lemmatization, we have to download WordNet, a large lexical database of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a little program to lemmatize the same `input_words` as above. Use the `lemmatize`-method from the `WordNetLemmatizer`-class. This method has two parameters: the first parameter is the word to be lemmatized, the second parameter is the type of output (pos='n' for a noun lemma, pos='v' for a verb lemma). The output should be something like this:\n",
    "\n",
    "```\n",
    "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
    " ===========================================================================\n",
    "                 writing                 writing                   write\n",
    "             connections              connection             connections\n",
    "               connected               connected                 connect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "             connections              connection             connections\n",
      "               connected               connected                 connect\n",
      "              connecting              connecting                 connect\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                  calves                    calf                   calve\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the noun lemmatizer works differently than the verb lemmatizer when it\n",
    "comes to words like writing or calves. If you compare these outputs to stemmer outputs, you\n",
    "will see that there are differences too. The lemmatizer outputs are all meaningful whereas\n",
    "stemmer outputs may or may not be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS Tagging\n",
    "\n",
    "The target of Part-of-Speech (POS) Tagging is to identify the grammatical group of a given word, whether it is a noun, pronoun, adjective, verb, adverb, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "We will use spaCy, a different Python library for NLP because it gives better results than NLTK for POS Tagging and Named Entity Recognition. First install spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (4.51.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: setuptools in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (44.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: setuptools in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (44.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=6915984092fdb6591d8559efd491ff9ae0b50644f6d25f1692764435d92fcb7d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-feufoq9q/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: nl_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-2.3.0/nl_core_news_sm-2.3.0.tar.gz#egg=nl_core_news_sm==2.3.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from nl_core_news_sm==2.3.0) (2.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (4.51.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (44.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->nl_core_news_sm==2.3.0) (1.25.11)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('nl_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next install the English model (restart the Kernel afterwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from en-core-web-sm==2.2.0) (2.3.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.51.0)\n",
      "Requirement already satisfied: setuptools in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (44.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (7.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.25.11)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019122 sha256=daf1a9635e1085d58fef5b183df34379d6e1621fbf97511a026bf2ca4f9cadc1\n",
      "  Stored in directory: /home/yori/.cache/pip/wheels/fc/31/e9/092e6f05b2817c9cb45804a3d1bf2b9bf6575742c01819337c\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.3.1\n",
      "    Uninstalling en-core-web-sm-2.3.1:\n",
      "      Successfully uninstalled en-core-web-sm-2.3.1\n",
      "Successfully installed en-core-web-sm-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the core spaCy English model and create a spaCy document that we will be using to perform Part-of-Speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yori/Documents/School/3ITF/AI/envs/AI/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaCy document object has several attributes that can be used to perform a variety of tasks. For instance, to print the text of the document, the text attribute is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play football. I hated it in my childhood though.\n"
     ]
    }
   ],
   "source": [
    "print(sen.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the pos_ attribute returns the POS tag. And finally, to get the explanation of the POS tag, we can use the spacy.explain() method and pass it the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hated\n",
      "VERB\n",
      "verb, past tense\n"
     ]
    }
   ],
   "source": [
    "print(sen[7])\n",
    "print(sen[7].pos_)\n",
    "print(spacy.explain(sen[7].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print all the POS tags (we've improved the readability by adding 12 spaces between the text and the POS tag and then another 10 spaces between the POS tags and the explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON       pronoun, personal\n",
      "like         VERB       verb, non-3rd person singular present\n",
      "to           PART       infinitival \"to\"\n",
      "play         VERB       verb, base form\n",
      "football     NOUN       noun, singular or mass\n",
      ".            PUNCT      punctuation mark, sentence closer\n",
      "I            PRON       pronoun, personal\n",
      "hated        VERB       verb, past tense\n",
      "it           PRON       pronoun, personal\n",
      "in           ADP        conjunction, subordinating or preposition\n",
      "my           PRON       pronoun, possessive\n",
      "childhood    NOUN       noun, singular or mass\n",
      "though       ADV        adverb\n",
      ".            PUNCT      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {spacy.explain(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing about spaCy is, that you can use the dependency visualizer to show Part-of-Speech tags and syntactic dependencies. Maybe you can try some other sentences to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b57e330709334e5ea1f8361afad903ea-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">play</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">football.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">hated</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">childhood</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">though.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,266.5 L1453.0,254.5 1437.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-7\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,266.5 L1803.0,254.5 1787.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b57e330709334e5ea1f8361afad903ea-0-9\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,2.0 1975.0,2.0 1975.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b57e330709334e5ea1f8361afad903ea-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,266.5 L1983.0,254.5 1967.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")\n",
    "displacy.render(sen, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging in Dutch - Exercise\n",
    "\n",
    "POS Tagging can be done in Dutch as well. You will probably have to install the Dutch model.\n",
    "\n",
    "Use this sentences as input: \"De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.\" The output should be as follows:\n",
    "\n",
    "```\n",
    "De           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "concentratie NOUN       N|soort|ev|neut__Number=Sing\n",
    "broeikasgassen ADP        Prep|voor__AdpType=Prep\n",
    "die          PRON       Pron|aanw|neut|attr__PronType=Dem\n",
    "bijdragen    NOUN       N|soort|mv|neut__Number=Plur\n",
    "aan          ADP        Prep|voor__AdpType=Prep\n",
    "de           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "verandering  NOUN       N|soort|ev|neut__Number=Sing\n",
    "van          ADP        Prep|voor__AdpType=Prep\n",
    "het          DET        Art|bep|onzijd|neut__Definite=Def|Gender=Neut|PronType=Art\n",
    "klimaat      NOUN       N|soort|ev|neut__Number=Sing\n",
    ",            PUNCT      Punc|komma__PunctType=Comm\n",
    "heeft        VERB       V|hulp|ott|3|ev__Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
    "opnieuw      ADV        Adv|gew|geenfunc|stell|onverv__Degree=Pos\n",
    "een          DET        Art|onbep|zijdofonzijd|neut__Definite=Ind|Number=Sing|PronType=Art\n",
    "recordhoogte NOUN       N|soort|ev|neut__Number=Sing\n",
    "bereikt      VERB       V|trans|verldw|onverv__Subcat=Tran|Tense=Past|VerbForm=Part\n",
    ".            PUNCT      Punc|punt__PunctType=Peri\n",
    "\n",
    "```\n",
    "\n",
    "It is not possible to explain the POS tag in Dutch. Just use tag_ in the third column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-2.2.0/nl_core_news_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "sen = sp('De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De           DET        LID|bep|stan|rest__Definite=Def\n",
      "concentratie NOUN       N|soort|ev|basis|zijd|stan__Gender=Com|Number=Sing\n",
      "broeikasgassen NOUN       N|soort|mv|basis__Number=Plur\n",
      "die          DET        VNW|aanw|det|stan|prenom|zonder|rest\n",
      "bijdragen    NOUN       N|soort|mv|basis__Number=Plur\n",
      "aan          ADP        VZ|init\n",
      "de           DET        LID|bep|stan|rest__Definite=Def\n",
      "verandering  NOUN       N|soort|ev|basis|zijd|stan__Gender=Com|Number=Sing\n",
      "van          ADP        VZ|init\n",
      "het          DET        LID|bep|stan|evon__Definite=Def\n",
      "klimaat      NOUN       N|soort|ev|basis|onz|stan__Gender=Neut|Number=Sing\n",
      ",            SYM        LET\n",
      "heeft        AUX        WW|pv|tgw|met-t__Number=Sing|Tense=Pres|VerbForm=Fin\n",
      "opnieuw      ADV        BW\n",
      "een          DET        LID|onbep|stan|agr__Definite=Ind\n",
      "recordhoogte NOUN       N|soort|ev|basis|zijd|stan__Gender=Com|Number=Sing\n",
      "bereikt      VERB       WW|vd|vrij|zonder__VerbForm=Part\n",
      ".            SYM        LET\n"
     ]
    }
   ],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition refers to the identification of words in a sentence as an entity e.g. the name of a person, place, organization, etc. Let's see how the spaCy library performs Named Entity Recognition. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Manchester United, Harry Kane, $90 million)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million.')\n",
    "\n",
    "print(sen.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that three named entities were identified. To see the detail of each named entity, you can use the text, label, and the spacy.explain method which takes the entity object as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United - ORG - Companies, agencies, institutions, etc.\n",
      "Harry Kane - PERSON - People, including fictional\n",
      "$90 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in sen.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the POS tags, we can also view named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " wants \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million. David wants 100 Million Dollars.')\n",
    "displacy.render(sen, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
