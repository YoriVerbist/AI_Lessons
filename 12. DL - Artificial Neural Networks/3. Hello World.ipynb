{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "\n",
    "We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify hand-written digits. The dataset we will use is the MNIST handwritten digit classification, a classic dataset in the deep learning community. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning programs for computer vision.\n",
    "\n",
    "Each image in the MNIST dataset is 28x28 and contains a centered, grayscale digit. We’ll flatten each 28x28 into a 784 dimensional vector, which we’ll use as input to our neural network. Our output will be one of 10 possible categories (0 to 9).\n",
    "\n",
    "<img src=\"./resources/mnist-examples.png\"  style=\"height: 315px\"/>\n",
    "\n",
    "Let's first install the packages we'll need. Note: we need to install TensorFlow because we're going to run Keras on a TensorFlow backend (i.e. TensorFlow will power Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Collecting mnist\n",
      "  Using cached mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.3-cp38-cp38-win_amd64.whl (31.4 MB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting numpy>=1.9.1\n",
      "  Using cached numpy-1.19.2-cp38-cp38-win_amd64.whl (13.0 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.3.1-cp38-cp38-win_amd64.whl (219 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Processing c:\\users\\yoriv\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.1-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Processing c:\\users\\yoriv\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\\wrapt-1.12.1-py3-none-any.whl\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.13.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.2-py3-none-any.whl (95 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.0.post20201006)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: numpy, scipy, h5py, pyyaml, keras, tensorflow-estimator, google-pasta, termcolor, astunparse, grpcio, wrapt, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, idna, chardet, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, markdown, werkzeug, absl-py, tensorboard-plugin-wit, tensorboard, keras-preprocessing, gast, opt-einsum, tensorflow, mnist\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 chardet-3.0.4 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.33.1 h5py-2.10.0 idna-2.10 keras-2.4.3 keras-preprocessing-1.1.2 markdown-3.3.2 mnist-0.2.2 numpy-1.19.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-5.3.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.5.3 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install keras tensorflow mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.3.1-cp38-cp38-win_amd64.whl (344.2 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: gast==0.3.3 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (0.10.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Collecting tensorflow-gpu-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_gpu_estimator-2.3.0-py2.py3-none-any.whl (474 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.33.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (3.13.0)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorflow-gpu) (2.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from protobuf>=3.9.2->tensorflow-gpu) (50.3.0.post20201006)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.3.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\users\\yoriv\\anaconda3\\envs\\gpu-test\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
      "Installing collected packages: tensorflow-gpu-estimator, numpy, tensorflow-gpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'D:\\\\Users\\\\yoriv\\\\anaconda3\\\\envs\\\\gpu-test\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages and classes\n",
    "\n",
    "We'll use tf.keras, a high-level API to build and train models in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ea7961546cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# helper libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the MNIST dataset\n",
    "\n",
    "The MNIST dataset is relatively small and is used to verify that an algorithm works as expected. \n",
    "\n",
    "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the MNIST directly from TensorFlow, just import and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the data\n",
    "\n",
    "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, there are 60,000 labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each label is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels. And the test set contains 10,000 image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape)\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess the data\n",
    "\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale these values to a range of 0 to 1 before feeding to the neural network model. Normalized pixel values make our network easier to train. For this, we divide the values by 255. It's important that the training set and the testing set are preprocessed in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 25 images from the training set and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the model\n",
    "\n",
    "The basic building block of a neural network is a layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up.\n",
    "\n",
    "After the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers. These are densely-connected, or fully-connected, neural layers. \n",
    "\n",
    "- The second layer has 128 nodes (or neurons) and uses the ReLU activation function. We could explain this function but it would involve a lot of math. Just remember that ReLU is a variant of the sigmoid function.\n",
    "\n",
    "- The third (and last) layer is a 10-node softmax layer (softmax turns arbitrary real values into probabilities). This layer returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image corresponds to the digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- Loss function — This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n",
    "- Optimizer — This is how the model is updated based on the data it sees and its loss function. We'll stick with a pretty good default: the Adam gradient-based optimizer.\n",
    "- Metrics — Used to monitor the training and testing steps. Since this is a classification problem, we'll just have Keras report on the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the model\n",
    "\n",
    "Training a model in Keras literally consists only of calling `fit()` and specifying some parameters. There are a lot of possible parameters, but we'll only manually supply a few:\n",
    "\n",
    "- The training data (images and labels), commonly known as X and y, respectively.\n",
    "- The number of epochs (iterations over the entire dataset) to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.98 (or 98%) on the training data. This doesn’t tell us much, though - we may be overfitting. The real challenge will be seeing how our model performs on our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate accuracy\n",
    "\n",
    "Evaluating the model is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. __This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on its training data.__\n",
    "\n",
    "Thus, our model achieves a 0.0817 test loss and 97.6% test accuracy (you might get slightly other values)! Not bad for your first neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make predictions\n",
    "\n",
    "With the model trained, we can use it to make predictions about the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different digits. We can see which digit has the highest confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model is most confident that this image is a seven. And we can check the image to see if this is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(test_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! Well done!\n",
    "\n",
    "## 10. Exercise\n",
    "\n",
    "Draw the first 20 __test images__ (5 images in a row). Below the image you print the actual digit (a) and the predicted digit (p). If they are the same the textcolor is green, red otherwise. You should get something like this:\n",
    "\n",
    "<img src=\"./resources/digits.png\"/>\n",
    "\n",
    "Since we achieved an accuracy of 97.6 % most of the labels should be green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Saving the Model\n",
    "\n",
    "Now that we have a working, trained model, we’ll save it to disk so we can load it back up anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('resources/helloworldmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reload the trained model whenever we want by rebuilding it and loading in the saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# load the model's saved weights\n",
    "model.load_weights('resources/helloworldmodel.h5')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
