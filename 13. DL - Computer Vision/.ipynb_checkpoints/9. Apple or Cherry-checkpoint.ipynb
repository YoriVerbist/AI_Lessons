{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple or Cherry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building the CNN\n",
    "\n",
    "First let us import all the required keras packages using which we are going to build our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In line 1, we've imported Sequential from keras.models, to initialise our neural network model as a sequential network.\n",
    "- In line 2, we’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN.\n",
    "- In line 3, we’ve imported MaxPooling2D from keras.layers, which is used for pooling operation. In line 4, we’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector. \n",
    "- And finally in line 5, we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network.\n",
    "- Line 6 will be explained later.\n",
    "\n",
    "We will create an object of the sequential class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution\n",
    "\n",
    "Let us now code the Convolution step, you will be surprised to see how easy it is to actually implement these complex operations in a single line of code in Python, thanks to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the object (model) and added a convolution layer by using the *Conv2D* function. The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image (RGB or Black and White) of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and *3* stands for RGB. The fourth argument is the activation function we want to use, here *relu* stands for a rectifier function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling\n",
    "\n",
    "Now, we need to perform pooling operation on the resultant feature maps we get after the convolution operation is done on an image. The primary aim of a pooling operation is to reduce the size of the images as much as possible. We start by taking our classifier object and add the pooling layer. We use Max Pooling on 2x2 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 13:53:19.744348 23308 deprecation_wrapper.py:119] From C:\\Users\\u0063152\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat step 1 and 2\n",
    "\n",
    "We will build a second convolution layer, Max Pooling and dropout layer with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening & Full connection\n",
    "\n",
    "It’s time for us to now convert all the pooled images into a continuous vector through Flattening. In the last step we need to create a fully connected layer, and to this layer we are going to connect the set of nodes we got after the flattening step, these nodes will act as an input layer to these fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(activation=\"relu\", units=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Dense is the function to add a fully connected layer, *units* is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "Now it’s time to initialise our output layer, which should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog. We will be using a sigmoid activation function for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(activation=\"sigmoid\", units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now that we have completed building our CNN model, it’s time to compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 13:53:36.025200 23308 deprecation.py:323] From C:\\Users\\u0063152\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the model summary to see what the whole model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are nearly 1 milion parameters that's need to be trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Fitting the CNN to the images\n",
    "\n",
    "### Step 1 - Data Augmentation\n",
    "\n",
    "While training your data, you need a lot of data to train upon. Suppose we have a limited number of images for our network. What to do now??\n",
    "\n",
    "You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.\n",
    "\n",
    "<img src=\"./resources/tennis.jpeg\"  style=\"height: 150px\"/>\n",
    "\n",
    "So, to get more data, we just need to make minor alterations to our existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 735 images belonging to 2 classes.\n",
      "Found 182 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('datasets/apple&cherry/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('datasets/apple&cherry/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can find the complete explanation of each of the above parameters, in the keras documentation page. But what you need to understand as a whole of whats happening above is that we are creating synthetic data out of the same images by performing different type of operations on these images like flipping, rotating, blurring, etc.\n",
    "\n",
    "One important parameter is target_size which is 64x64, the same as the input_shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Training our network\n",
    "\n",
    "Now lets fit the data to our model! Training the network might take a while! Meanwwile you can read on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 13:54:07.073641 23308 deprecation_wrapper.py:119] From C:\\Users\\u0063152\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.4820 - accuracy: 0.7610 - val_loss: 0.2493 - val_accuracy: 0.8901\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2896 - accuracy: 0.8776 - val_loss: 0.1990 - val_accuracy: 0.9231\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.2328 - accuracy: 0.9046 - val_loss: 0.2463 - val_accuracy: 0.9231\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.1712 - accuracy: 0.9321 - val_loss: 0.2181 - val_accuracy: 0.9341\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.1499 - accuracy: 0.9412 - val_loss: 0.2399 - val_accuracy: 0.9286\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.1116 - accuracy: 0.9609 - val_loss: 0.1274 - val_accuracy: 0.9396\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 0.2585 - val_accuracy: 0.9176\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0704 - accuracy: 0.9725 - val_loss: 0.0919 - val_accuracy: 0.9286\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0539 - accuracy: 0.9812 - val_loss: 0.0195 - val_accuracy: 0.9231\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0724 - accuracy: 0.9725 - val_loss: 0.0273 - val_accuracy: 0.9341\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.0039 - val_accuracy: 0.9670\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0493 - accuracy: 0.9809 - val_loss: 0.3621 - val_accuracy: 0.9670\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.3321 - val_accuracy: 0.9451\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 0.0404 - val_accuracy: 0.9560\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0288 - accuracy: 0.9903 - val_loss: 0.1573 - val_accuracy: 0.9396\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0351 - accuracy: 0.9887 - val_loss: 0.2432 - val_accuracy: 0.9615\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0295 - accuracy: 0.9900 - val_loss: 0.1527 - val_accuracy: 0.9615\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.0486 - val_accuracy: 0.9670\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.2857 - val_accuracy: 0.9286\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.0331 - accuracy: 0.9900 - val_loss: 0.0037 - val_accuracy: 0.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x187bc579240>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was 100, 100 (door rik)\n",
    "# Epoch 100/100\n",
    "# 100/100 [==============================] - 37s 368ms/step - loss: 0.3335 - accuracy: 0.8519 - val_loss: 0.6576 - val_accuracy: 0.7655\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 20,\n",
    "                    validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, an __epoch__ is a single step in training the neural network. __steps_per_epoch holds__ the number of training images that is used during every step. So we are using 100 images during each step and train the network in 20 steps. Especially the number of images is far to less, but otherwise it would take to long. You can try to modify these parameters yourself later (1000 might be better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret loss and accuracy?\n",
    "\n",
    "- The loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s) or epochs. So the lower the loss, the better a model (unless the model has over-fitted to the training data).\n",
    "\n",
    "- The loss is calculated on training (loss) and test (val_loss) data and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\n",
    "\n",
    "- __The loss value should be better after each epoch.__\n",
    "\n",
    "\n",
    "- The accuracy is also calculated for the training (accuracy) and the test (val_accuracy) data. The accuracy of a model for the test data, is usually determined after the model parameters are learned and fixed and no learning is taking place.\n",
    "\n",
    "- After each epoch, the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.\n",
    "\n",
    "- __If the model has a good accuracy but bad val_accuracy, it performs way better for the train data then for the test data, it is overfitted.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the weights\n",
    "\n",
    "Since it might take a while to train the model, after completing the fit, you can save the calculated weights and load them again the next time you want to use the model again.\n",
    "\n",
    "```python\n",
    "# save weights\n",
    "model.save_weights('saved_models/modelcats&dogs.h5')\n",
    "# load weights\n",
    "model.load_weights('saved_models/modelcats&dogs.h5')\n",
    "\n",
    "```\n",
    "\n",
    "You can load the weights of a model I've trained with an accuracy 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('saved_models/modelapple&cherry.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. - Making new predictions from our trained model\n",
    "\n",
    "Now lets test some random images. In the `singe_image` folder you will find some images of cats and dogs to test you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kers\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img(\"datasets/apple&cherry/single_image/kersen4.jpg\", target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = \"kers\"\n",
    "else:\n",
    "    prediction = \"appel\"\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_image holds the image that needs to be tested on the CNN. Once we have the test image, we will prepare the image to be sent into the model by converting its resolution to 64x64 as the model only excepts that resolution. Then we are using predict() method on our classifier object to get the prediction. As the prediction will be in a binary form, we will be receiving either a 1 or 0, which will represent a dog or a cat respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it is not 100% accurate but it will give correct predictions most of the times. Try adding more convolutional and pooling layers, play with the number of nodes and epochs, and you might get high accuracy result.\n",
    "\n",
    "Maybe you've got a cat or a dog yourself? Make a picture of it and see if the model can predict if it's a cat or dog? You can even try it with your own image and see what it predicts. Whether you look close to a dog or a cat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
