{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple or Cherry?\n",
    "\n",
    "In this Notebook we will finally be solving an image classification problem, where our goal will be to tell which class an  input image belongs to. The way we are going to achieve it is by training an Convolutional Neural Network on few thousand images of cats and dogs and make the NN (Neural Network) learn to predict which class the image belongs to, next time it sees an image having a cat or dog in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building the CNN\n",
    "\n",
    "First let us import all the required keras packages using which we are going to build our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In line 1, we've imported Sequential from keras.models, to initialise our neural network model as a sequential network.\n",
    "- In line 2, we’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN.\n",
    "- In line 3, we’ve imported MaxPooling2D from keras.layers, which is used for pooling operation. In line 4, we’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector. \n",
    "- And finally in line 5, we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network.\n",
    "- Line 6 will be explained later.\n",
    "\n",
    "We will create an object of the sequential class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the CNN (73% 62 %)\n",
    "model = Sequential() \n",
    "model.add(Conv2D(64, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(activation=\"relu\", units=128))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the CNN (68% 64%) 11/20 goed // na 200/40 91% 71% \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (64,64,3)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution\n",
    "\n",
    "Let us now code the Convolution step, you will be surprised to see how easy it is to actually implement these complex operations in a single line of code in Python, thanks to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the object (model) and added a convolution layer by using the *Conv2D* function. The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image (RGB or Black and White) of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and *3* stands for RGB. The fourth argument is the activation function we want to use, here *relu* stands for a rectifier function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling\n",
    "\n",
    "Now, we need to perform pooling operation on the resultant feature maps we get after the convolution operation is done on an image. The primary aim of a pooling operation is to reduce the size of the images as much as possible. We start by taking our classifier object and add the pooling layer. We use Max Pooling on 2x2 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat step 1 and 2\n",
    "\n",
    "We will build a second convolution layer, Max Pooling and dropout layer with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening & Full connection\n",
    "\n",
    "It’s time for us to now convert all the pooled images into a continuous vector through Flattening. In the last step we need to create a fully connected layer, and to this layer we are going to connect the set of nodes we got after the flattening step, these nodes will act as an input layer to these fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(activation=\"relu\", units=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Dense is the function to add a fully connected layer, *units* is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "Now it’s time to initialise our output layer, which should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog. We will be using a sigmoid activation function for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now that we have completed building our CNN model, it’s time to compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the model summary to see what the whole model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 32)        2432      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 64, 64, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 4,280,618\n",
      "Trainable params: 4,280,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are nearly 1 milion parameters that's need to be trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Fitting the CNN to the images\n",
    "\n",
    "### Step 1 - Data Augmentation\n",
    "\n",
    "While training your data, you need a lot of data to train upon. Suppose we have a limited number of images for our network. What to do now??\n",
    "\n",
    "You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.\n",
    "\n",
    "<img src=\"./resources/tennis.jpeg\"  style=\"height: 150px\"/>\n",
    "\n",
    "So, to get more data, we just need to make minor alterations to our existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3419 images belonging to 10 classes.\n",
      "Found 817 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('datasets/colruyt_all/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('datasets/colruyt_all/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can find the complete explanation of each of the above parameters, in the keras documentation page. But what you need to understand as a whole of whats happening above is that we are creating synthetic data out of the same images by performing different type of operations on these images like flipping, rotating, blurring, etc.\n",
    "\n",
    "One important parameter is target_size which is 64x64, the same as the input_shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Training our network\n",
    "\n",
    "Now lets fit the data to our model! Training the network might take a while! Meanwwile you can read on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "200/200 [==============================] - 115s 574ms/step - loss: 0.8705 - accuracy: 0.6935 - val_loss: 1.0189 - val_accuracy: 0.6353\n",
      "Epoch 2/40\n",
      "200/200 [==============================] - 124s 620ms/step - loss: 0.8446 - accuracy: 0.7018 - val_loss: 0.8897 - val_accuracy: 0.6683\n",
      "Epoch 3/40\n",
      "200/200 [==============================] - 126s 628ms/step - loss: 0.7923 - accuracy: 0.7185 - val_loss: 1.0642 - val_accuracy: 0.6756\n",
      "Epoch 4/40\n",
      "200/200 [==============================] - 126s 630ms/step - loss: 0.7277 - accuracy: 0.7396 - val_loss: 0.5021 - val_accuracy: 0.7050\n",
      "Epoch 5/40\n",
      "200/200 [==============================] - 128s 641ms/step - loss: 0.7022 - accuracy: 0.7504 - val_loss: 1.0662 - val_accuracy: 0.7075\n",
      "Epoch 6/40\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.6788 - accuracy: 0.7535 - val_loss: 0.5472 - val_accuracy: 0.7038\n",
      "Epoch 7/40\n",
      "200/200 [==============================] - 129s 647ms/step - loss: 0.6489 - accuracy: 0.7646 - val_loss: 0.8240 - val_accuracy: 0.7001\n",
      "Epoch 8/40\n",
      "200/200 [==============================] - 126s 632ms/step - loss: 0.6257 - accuracy: 0.7831 - val_loss: 1.5190 - val_accuracy: 0.7013\n",
      "Epoch 9/40\n",
      "200/200 [==============================] - 129s 643ms/step - loss: 0.5549 - accuracy: 0.8006 - val_loss: 1.4391 - val_accuracy: 0.6916\n",
      "Epoch 10/40\n",
      "200/200 [==============================] - 129s 645ms/step - loss: 0.5203 - accuracy: 0.8141 - val_loss: 0.7494 - val_accuracy: 0.7075\n",
      "Epoch 11/40\n",
      "200/200 [==============================] - 127s 635ms/step - loss: 0.5294 - accuracy: 0.8105 - val_loss: 1.0947 - val_accuracy: 0.6805\n",
      "Epoch 12/40\n",
      "200/200 [==============================] - 128s 638ms/step - loss: 0.4693 - accuracy: 0.8275 - val_loss: 1.7257 - val_accuracy: 0.7185\n",
      "Epoch 13/40\n",
      "200/200 [==============================] - 127s 633ms/step - loss: 0.4807 - accuracy: 0.8305 - val_loss: 1.5844 - val_accuracy: 0.7173\n",
      "Epoch 14/40\n",
      "200/200 [==============================] - 128s 640ms/step - loss: 0.4594 - accuracy: 0.8346 - val_loss: 0.5557 - val_accuracy: 0.6965\n",
      "Epoch 15/40\n",
      "200/200 [==============================] - 127s 636ms/step - loss: 0.4567 - accuracy: 0.8393 - val_loss: 1.1209 - val_accuracy: 0.6891\n",
      "Epoch 16/40\n",
      "200/200 [==============================] - 128s 638ms/step - loss: 0.4310 - accuracy: 0.8502 - val_loss: 1.3113 - val_accuracy: 0.7185\n",
      "Epoch 17/40\n",
      "200/200 [==============================] - 129s 646ms/step - loss: 0.3944 - accuracy: 0.8574 - val_loss: 1.6649 - val_accuracy: 0.7173\n",
      "Epoch 18/40\n",
      "200/200 [==============================] - 128s 640ms/step - loss: 0.4158 - accuracy: 0.8507 - val_loss: 0.7817 - val_accuracy: 0.7185\n",
      "Epoch 19/40\n",
      "200/200 [==============================] - 128s 638ms/step - loss: 0.3651 - accuracy: 0.8704 - val_loss: 1.9359 - val_accuracy: 0.7209\n",
      "Epoch 20/40\n",
      "200/200 [==============================] - 128s 639ms/step - loss: 0.3867 - accuracy: 0.8635 - val_loss: 0.9960 - val_accuracy: 0.7038\n",
      "Epoch 21/40\n",
      "200/200 [==============================] - 128s 639ms/step - loss: 0.3547 - accuracy: 0.8752 - val_loss: 1.1954 - val_accuracy: 0.7185\n",
      "Epoch 22/40\n",
      "200/200 [==============================] - 128s 638ms/step - loss: 0.3503 - accuracy: 0.8743 - val_loss: 1.4630 - val_accuracy: 0.7283\n",
      "Epoch 23/40\n",
      "200/200 [==============================] - 127s 637ms/step - loss: 0.3462 - accuracy: 0.8762 - val_loss: 2.5915 - val_accuracy: 0.7062\n",
      "Epoch 24/40\n",
      "200/200 [==============================] - 131s 654ms/step - loss: 0.3300 - accuracy: 0.8751 - val_loss: 0.6835 - val_accuracy: 0.6732\n",
      "Epoch 25/40\n",
      "200/200 [==============================] - 128s 641ms/step - loss: 0.3228 - accuracy: 0.8876 - val_loss: 0.7477 - val_accuracy: 0.7466\n",
      "Epoch 26/40\n",
      "200/200 [==============================] - 128s 642ms/step - loss: 0.3355 - accuracy: 0.8824 - val_loss: 1.3537 - val_accuracy: 0.7136\n",
      "Epoch 27/40\n",
      "200/200 [==============================] - 128s 640ms/step - loss: 0.2989 - accuracy: 0.8981 - val_loss: 1.3210 - val_accuracy: 0.7136\n",
      "Epoch 28/40\n",
      "200/200 [==============================] - 128s 640ms/step - loss: 0.3212 - accuracy: 0.8862 - val_loss: 0.6969 - val_accuracy: 0.7124\n",
      "Epoch 29/40\n",
      "200/200 [==============================] - 128s 641ms/step - loss: 0.2785 - accuracy: 0.9017 - val_loss: 1.6316 - val_accuracy: 0.7307\n",
      "Epoch 30/40\n",
      "200/200 [==============================] - 128s 640ms/step - loss: 0.3003 - accuracy: 0.8897 - val_loss: 2.5883 - val_accuracy: 0.7062\n",
      "Epoch 31/40\n",
      "200/200 [==============================] - 127s 637ms/step - loss: 0.2823 - accuracy: 0.9005 - val_loss: 1.4391 - val_accuracy: 0.7197\n",
      "Epoch 32/40\n",
      "200/200 [==============================] - 130s 649ms/step - loss: 0.2819 - accuracy: 0.9034 - val_loss: 2.2992 - val_accuracy: 0.7136\n",
      "Epoch 33/40\n",
      "200/200 [==============================] - 30711s 154s/step - loss: 0.2912 - accuracy: 0.8972 - val_loss: 1.7608 - val_accuracy: 0.7038\n",
      "Epoch 34/40\n",
      "200/200 [==============================] - 112s 561ms/step - loss: 0.2767 - accuracy: 0.9089 - val_loss: 1.4539 - val_accuracy: 0.6854\n",
      "Epoch 35/40\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.2536 - accuracy: 0.9102 - val_loss: 1.5267 - val_accuracy: 0.7075\n",
      "Epoch 36/40\n",
      "200/200 [==============================] - 122s 612ms/step - loss: 0.2573 - accuracy: 0.9073 - val_loss: 1.0977 - val_accuracy: 0.7124\n",
      "Epoch 37/40\n",
      "200/200 [==============================] - 128s 641ms/step - loss: 0.2713 - accuracy: 0.9095 - val_loss: 0.7563 - val_accuracy: 0.7234\n",
      "Epoch 38/40\n",
      "200/200 [==============================] - 127s 633ms/step - loss: 0.2885 - accuracy: 0.8998 - val_loss: 0.6907 - val_accuracy: 0.7013\n",
      "Epoch 39/40\n",
      "200/200 [==============================] - 118s 589ms/step - loss: 0.2404 - accuracy: 0.9155 - val_loss: 1.1015 - val_accuracy: 0.7319\n",
      "Epoch 40/40\n",
      "200/200 [==============================] - 121s 606ms/step - loss: 0.2543 - accuracy: 0.9164 - val_loss: 0.3322 - val_accuracy: 0.7160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x118bf0e92b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was 100, 100 (door rik)\n",
    "# Epoch 100/100\n",
    "# 100/100 [==============================] - 37s 368ms/step - loss: 0.3335 - accuracy: 0.8519 - val_loss: 0.6576 - val_accuracy: 0.7655\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch = 200,\n",
    "                    epochs = 40,\n",
    "                    validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, an __epoch__ is a single step in training the neural network. __steps_per_epoch holds__ the number of training images that is used during every step. So we are using 100 images during each step and train the network in 20 steps. Especially the number of images is far to less, but otherwise it would take to long. You can try to modify these parameters yourself later (1000 might be better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret loss and accuracy?\n",
    "\n",
    "- The loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s) or epochs. So the lower the loss, the better a model (unless the model has over-fitted to the training data).\n",
    "\n",
    "- The loss is calculated on training (loss) and test (val_loss) data and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\n",
    "\n",
    "- __The loss value should be better after each epoch.__\n",
    "\n",
    "\n",
    "- The accuracy is also calculated for the training (accuracy) and the test (val_accuracy) data. The accuracy of a model for the test data, is usually determined after the model parameters are learned and fixed and no learning is taking place.\n",
    "\n",
    "- After each epoch, the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.\n",
    "\n",
    "- __If the model has a good accuracy but bad val_accuracy, it performs way better for the train data then for the test data, it is overfitted.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the weights\n",
    "\n",
    "Since it might take a while to train the model, after completing the fit, you can save the calculated weights and load them again the next time you want to use the model again.\n",
    "\n",
    "```python\n",
    "# save weights\n",
    "model.save_weights('saved_models/modelcats&dogs.h5')\n",
    "# load weights\n",
    "model.load_weights('saved_models/modelcats&dogs.h5')\n",
    "\n",
    "```\n",
    "\n",
    "You can load the weights of a model I've trained with an accuracy 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('saved_models/modelall10fruits.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. - Making new predictions from our trained model\n",
    "\n",
    "Now lets test some random images. In the `singe_image` folder you will find some images of cats and dogs to test you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "{'apple': 0, 'cherry': 1, 'kiwi': 2}\n",
      "kiwi\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img(\"datasets/colruyt/single_images/img05.jpg\", target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "print(result[0])\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = \"appel\"\n",
    "if result[0][1] == 1:\n",
    "    prediction = \"kers\"\n",
    "if result[0][2] == 1:\n",
    "    prediction = \"kiwi\"\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/colruyt/single_images\\img01.jpg peach\n",
      "datasets/colruyt/single_images\\img02.jpg cherry\n",
      "datasets/colruyt/single_images\\img03.jpg apple\n",
      "datasets/colruyt/single_images\\img04.jpg cherry\n",
      "datasets/colruyt/single_images\\img05.jpg grape\n",
      "datasets/colruyt/single_images\\img06.jpg peach\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a126c12f28b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnumber\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import glob\n",
    "\n",
    "def classify(imagefile):\n",
    "    test_image = image.load_img(imagefile, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model.predict(test_image)\n",
    "    \n",
    "    for i in range(len(result[0])):\n",
    "        if result[0][i] == 1:\n",
    "            return list(training_set.class_indices)[i]\n",
    "\n",
    "img_folder = 'datasets/colruyt/single_images/'\n",
    "\n",
    "number = 0\n",
    "\n",
    "file_list = glob.glob(img_folder + '*.jpg') # returns list of files\n",
    "for file in file_list:\n",
    "    result = classify(file)\n",
    "    print(file + \" \" + result)\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_image holds the image that needs to be tested on the CNN. Once we have the test image, we will prepare the image to be sent into the model by converting its resolution to 64x64 as the model only excepts that resolution. Then we are using predict() method on our classifier object to get the prediction. As the prediction will be in a binary form, we will be receiving either a 1 or 0, which will represent a dog or a cat respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it is not 100% accurate but it will give correct predictions most of the times. Try adding more convolutional and pooling layers, play with the number of nodes and epochs, and you might get high accuracy result.\n",
    "\n",
    "Maybe you've got a cat or a dog yourself? Make a picture of it and see if the model can predict if it's a cat or dog? You can even try it with your own image and see what it predicts. Whether you look close to a dog or a cat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
