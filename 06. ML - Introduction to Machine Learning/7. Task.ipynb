{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task ML. My recommender system: ```Subreddits```\n",
    "\n",
    "- Student name: Yori Verbist\n",
    "- Number: r0697651\n",
    "\n",
    "First find a topic which you can develop a recommender system for. Don't use a film recommender system this time, try to be a little more original. Maybe build a recommender system for TV series, holiday destinations, computer games, favorite (exotic) dishes or recipies, city trip destinations, computer games, ... Use your imagination, you should definitely be able to find something funny. List at least 20 different items on the chosen subject. \n",
    "\n",
    "```Reddit Subreddits, because there are a lot of them so there will always be some you have never heard of before.```\n",
    "\n",
    "Next think of your target group. Who do you want to set up the recommender system for? For friends, family, classmates, ...?\n",
    "Design and develop a tool to collect the data for your recommender system. A possible tool is *Google Forms*, but maybe you know a better way. Now, using the tool, ask your target group, to rate the (at least) 20 items. You should also at least have 20 different respondents.\n",
    "\n",
    "__Make sure you provide the option \"not yet tried\" or \"no rating\" in your survey. If everyone gives a rating for all the data, there is nothing to recommend.__\n",
    "\n",
    "<img src=\"./recommender/form.png\"  style=\"height: 500px\"/>\n",
    "\n",
    "The following step is to convert the data in the correct json-format. Insert below a part of the json-file:\n",
    "\n",
    "```{\n",
    "    \"Tijdstempel\": {\n",
    "        \"0\": \"17-10-2020 17:39:09\",\n",
    "        \"1\": \"17-10-2020 17:39:45\",\n",
    "        \"2\": \"17-10-2020 17:40:48\",\n",
    "        \"3\": \"17-10-2020 17:46:00\",\n",
    "        \"4\": \"17-10-2020 17:51:51\",\n",
    "        \"5\": \"17-10-2020 17:51:53\",\n",
    "        \"6\": \"17-10-2020 18:08:21\",\n",
    "        \"7\": \"17-10-2020 19:55:59\",\n",
    "        \"8\": \"17-10-2020 21:27:32\",\n",
    "        \"9\": \"17-10-2020 21:39:46\",\n",
    "        \"10\": \"18-10-2020 12:47:46\",\n",
    "        \"11\": \"18-10-2020 18:09:02\",\n",
    "        \"12\": \"19-10-2020 8:47:22\",\n",
    "        \"13\": \"19-10-2020 8:52:25\",\n",
    "        \"14\": \"19-10-2020 8:54:21\",\n",
    "        \"15\": \"19-10-2020 8:57:40\",\n",
    "        \"16\": \"19-10-2020 9:08:47\",\n",
    "        \"17\": \"19-10-2020 9:14:09\",\n",
    "        \"18\": \"19-10-2020 9:16:11\",\n",
    "        \"19\": \"19-10-2020 9:26:58\"\n",
    "    },\n",
    "    \"r\\/math\": {\n",
    "        \"0\": \"Don't know\",\n",
    "        \"1\": \"Don't know\",\n",
    "        \"2\": \"Don't know\",\n",
    "        \"3\": \"1\",\n",
    "        \"4\": \"1\",\n",
    "        \"5\": \"3\",\n",
    "        \"6\": \"Don't know\",\n",
    "        \"7\": \"Don't know\",\n",
    "        \"8\": \"3\",\n",
    "        \"9\": \"Don't know\",```\n",
    "\n",
    "Once the data is collected, we can start using it to recommend items to our users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the code library with the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('recommender/collaborative_filtering.py')\n",
    "\n",
    "from recommender.collaborative_filtering import euclidean_score, pearson_score, find_similar_users, get_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the json file and calculate the similarity score between two of your respondents using the two different methods. Does it make sense? Explain briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = 'recommender/subreddits.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(ratings_file)\n",
    "df.to_json('recommender/subreddits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(df := pd.read_json('recommender/subreddits.json'))\n",
    "import json\n",
    "\n",
    "def parse_json(data):\n",
    "    users = [\"User\" + str(i) for i in range(len(data[\"r/math\"]))]\n",
    "    ratings = {}\n",
    "    #print(data.keys())\n",
    "    for i in range(len(users)):\n",
    "        ratings[users[i]] = {}\n",
    "        for value in data.keys():\n",
    "            if value == 'Tijdstempel':\n",
    "                pass\n",
    "            elif data[value][str(i)] in ['1', '2', '3']:\n",
    "                ratings[users[i]][str(value)] = int(data[value][str(i)])\n",
    "            elif data[value][str(i)] == \"Don't know\":\n",
    "                pass\n",
    "            else:\n",
    "                ratings[users[i]][str(value)] = data[value][str(i)]\n",
    "            \n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User0': {'r/programming': 2, 'r/aww': 1, 'r/EarthPorn': 1, 'r/AskReddit': 3, 'r/LifeProTips': 2, 'r/ProgrammerHumor': 2, 'r/wallstreetbets': 1, 'r/wholesomememes': 2, 'r/explainlikeimfive': 3, 'r/dankmemes': 3, 'r/todayilearned': 3}, 'User1': {'r/programming': 2, 'r/aww': 2, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 1, 'r/AskReddit': 1, 'r/LifeProTips': 1, 'r/ProgrammerHumor': 2, 'r/Tinder': 1, 'r/wallstreetbets': 2, 'r/wholesomememes': 1, 'r/me_irl': 1, 'r/IdiotsInCars': 3, 'r/oddlysatisfying': 2, 'r/explainlikeimfive': 2, 'r/dankmemes': 1, 'r/todayilearned': 1}, 'User2': {'r/learnmath': 2, 'r/aww': 3, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 3, 'r/AskReddit': 1, 'r/LifeProTips': 1, 'r/Tinder': 3, 'r/wholesomememes': 3, 'r/me_irl': 3, 'r/IdiotsInCars': 3, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 3, 'r/dankmemes': 3, 'r/todayilearned': 3}, 'User3': {'r/math': 1, 'r/learnmath': 1, 'r/programming': 1, 'r/learnprogramming': 1, 'r/aww': 3, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 3, 'r/AskReddit': 1, 'r/LifeProTips': 2, 'r/ProgrammerHumor': 2, 'r/Tinder': 3, 'r/wholesomememes': 3, 'r/me_irl': 2, 'r/IdiotsInCars': 3, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 3, 'r/dankmemes': 3, 'r/todayilearned': 3}, 'User4': {'r/math': 1, 'r/aww': 3, 'r/NatureIsFuckingLit': 2, 'r/EarthPorn': 2, 'r/AskReddit': 2, 'r/LifeProTips': 1, 'r/Tinder': 2, 'r/wholesomememes': 1, 'r/me_irl': 1, 'r/IdiotsInCars': 2, 'r/CozyPlaces': 3, 'r/oddlysatisfying': 2, 'r/explainlikeimfive': 3, 'r/dankmemes': 3, 'r/todayilearned': 3}, 'User5': {'r/math': 3, 'r/learnmath': 1, 'r/programming': 3, 'r/learnprogramming': 1, 'r/aww': 3, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 2, 'r/AskReddit': 2, 'r/LifeProTips': 1, 'r/ProgrammerHumor': 2, 'r/Tinder': 1, 'r/wallstreetbets': 3, 'r/wholesomememes': 2, 'r/IdiotsInCars': 3, 'r/CozyPlaces': 3, 'r/oddlysatisfying': 2}, 'User6': {'r/EarthPorn': 1, 'r/AskReddit': 3, 'r/LifeProTips': 3, 'r/ProgrammerHumor': 2, 'r/Tinder': 2, 'r/wallstreetbets': 1, 'r/wholesomememes': 1, 'r/IdiotsInCars': 1, 'r/oddlysatisfying': 1, 'r/explainlikeimfive': 1, 'r/dankmemes': 3, 'r/todayilearned': 1}, 'User7': {'r/programming': 2, 'r/aww': 2, 'r/AskReddit': 1, 'r/ProgrammerHumor': 2, 'r/wholesomememes': 2, 'r/me_irl': 1}, 'User8': {'r/math': 3, 'r/learnmath': 2, 'r/AskReddit': 2, 'r/LifeProTips': 1, 'r/Tinder': 2, 'r/todayilearned': 2}, 'User9': {'r/programming': 2, 'r/learnprogramming': 2, 'r/aww': 1, 'r/NatureIsFuckingLit': 2, 'r/EarthPorn': 1, 'r/AskReddit': 1, 'r/LifeProTips': 3, 'r/ProgrammerHumor': 2, 'r/Tinder': 3, 'r/wallstreetbets': 2, 'r/wholesomememes': 3, 'r/me_irl': 2, 'r/IdiotsInCars': 2, 'r/CozyPlaces': 1, 'r/oddlysatisfying': 3, 'r/dankmemes': 3}, 'User10': {'r/math': 1, 'r/learnmath': 1, 'r/programming': 3, 'r/learnprogramming': 3, 'r/aww': 2, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 3, 'r/AskReddit': 3, 'r/LifeProTips': 2, 'r/ProgrammerHumor': 2, 'r/Tinder': 3, 'r/wallstreetbets': 2, 'r/wholesomememes': 2, 'r/me_irl': 2, 'r/IdiotsInCars': 3, 'r/CozyPlaces': 1, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 2, 'r/dankmemes': 3, 'r/todayilearned': 2}, 'User11': {'r/math': 2, 'r/learnmath': 3, 'r/aww': 1, 'r/NatureIsFuckingLit': 1, 'r/AskReddit': 2, 'r/LifeProTips': 1, 'r/wallstreetbets': 1, 'r/me_irl': 1, 'r/CozyPlaces': 2, 'r/oddlysatisfying': 3, 'r/todayilearned': 2}, 'User12': {'r/math': 1, 'r/programming': 2, 'r/aww': 1, 'r/NatureIsFuckingLit': 2, 'r/EarthPorn': 3, 'r/ProgrammerHumor': 2, 'r/Tinder': 1, 'r/wallstreetbets': 3, 'r/wholesomememes': 1, 'r/me_irl': 2, 'r/IdiotsInCars': 3, 'r/explainlikeimfive': 2, 'r/dankmemes': 1, 'r/todayilearned': 3}, 'User13': {'r/learnmath': 2, 'r/programming': 2, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 1, 'r/AskReddit': 2, 'r/Tinder': 1, 'r/wholesomememes': 3, 'r/me_irl': 1, 'r/CozyPlaces': 2, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 1, 'r/dankmemes': 1, 'r/todayilearned': 3}, 'User14': {'r/math': 3, 'r/learnmath': 2, 'r/programming': 3, 'r/learnprogramming': 2, 'r/AskReddit': 3, 'r/LifeProTips': 1, 'r/ProgrammerHumor': 3, 'r/Tinder': 2, 'r/wallstreetbets': 3, 'r/me_irl': 1, 'r/IdiotsInCars': 3, 'r/dankmemes': 2, 'r/todayilearned': 3}, 'User15': {'r/programming': 2, 'r/EarthPorn': 2, 'r/AskReddit': 2, 'r/ProgrammerHumor': 2, 'r/Tinder': 1, 'r/wallstreetbets': 3, 'r/wholesomememes': 3, 'r/IdiotsInCars': 2, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 1, 'r/dankmemes': 3}, 'User16': {'r/aww': 3, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 3, 'r/AskReddit': 2, 'r/Tinder': 2, 'r/wholesomememes': 2, 'r/me_irl': 1, 'r/CozyPlaces': 3, 'r/oddlysatisfying': 3, 'r/todayilearned': 3}, 'User17': {'r/math': 3, 'r/learnmath': 3, 'r/aww': 3, 'r/AskReddit': 3, 'r/LifeProTips': 2, 'r/me_irl': 1, 'r/dankmemes': 3, 'r/todayilearned': 3}, 'User18': {'r/math': 2, 'r/learnmath': 3, 'r/programming': 1, 'r/learnprogramming': 3, 'r/aww': 2, 'r/NatureIsFuckingLit': 1, 'r/EarthPorn': 2, 'r/AskReddit': 3, 'r/ProgrammerHumor': 3, 'r/Tinder': 2, 'r/wallstreetbets': 1, 'r/me_irl': 3, 'r/IdiotsInCars': 1, 'r/CozyPlaces': 2, 'r/oddlysatisfying': 2, 'r/explainlikeimfive': 2, 'r/dankmemes': 2, 'r/todayilearned': 2}, 'User19': {'r/programming': 3, 'r/learnprogramming': 3, 'r/aww': 3, 'r/NatureIsFuckingLit': 3, 'r/EarthPorn': 2, 'r/ProgrammerHumor': 3, 'r/wholesomememes': 2, 'r/me_irl': 1, 'r/IdiotsInCars': 3, 'r/CozyPlaces': 3, 'r/oddlysatisfying': 3, 'r/explainlikeimfive': 2, 'r/dankmemes': 1, 'r/todayilearned': 3}}\n"
     ]
    }
   ],
   "source": [
    "with open('recommender/subreddits.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data = parse_json(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Euclidean score:\n",
      "0.16139047779640892\n",
      "\n",
      "Pearson score:\n",
      "0.30785964799347953\n"
     ]
    }
   ],
   "source": [
    "user1 = \"User1\"\n",
    "user2 = \"User2\"\n",
    "\n",
    "print(\"\\nEuclidean score:\")\n",
    "print(euclidean_score(data, user1, user2))\n",
    "    \n",
    "print(\"\\nPearson score:\")\n",
    "print(pearson_score(data, user1, user2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find - given a respondent - five similar respondents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users similar to User19:\n",
      "\n",
      "User\t\t\tPearson similarity score\n",
      "------------------------------------------------\n",
      "User16 \t\t\t 0.89\n",
      "User7 \t\t\t 0.87\n",
      "User14 \t\t\t 0.81\n",
      "User13 \t\t\t 0.72\n",
      "User1 \t\t\t 0.65\n"
     ]
    }
   ],
   "source": [
    "user = \"User19\"\n",
    "\n",
    "print('\\nUsers similar to ' + user + ':\\n')\n",
    "similar_users = find_similar_users(data, user, 5) \n",
    "print('User\\t\\t\\tPearson similarity score')\n",
    "print('-'*48)\n",
    "for item in similar_users:\n",
    "    print(item[0], '\\t\\t\\t', round(float(item[1]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your recommendation system to give some recommendations for one respondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Subreddit recommendations for User19:\n",
      "1. r/learnmath\n",
      "2. r/math\n",
      "3. r/wallstreetbets\n",
      "4. r/AskReddit\n",
      "5. r/Tinder\n",
      "6. r/LifeProTips\n"
     ]
    }
   ],
   "source": [
    "print(\"\\Subreddit recommendations for \" + user + \":\")\n",
    "movies = get_recommendations(data, user) \n",
    "for i, movie in enumerate(movies):\n",
    "    print(str(i+1) + '. ' + movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the respondents in a table. For every respondent, you show the most similar respondent and the first recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print/export your Jupyter Notebook to a pdf file and upload it using Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
